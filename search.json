[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2025-02-14-vocalpy-0.10.0/index.html",
    "href": "posts/2025-02-14-vocalpy-0.10.0/index.html",
    "title": "VocalPy 0.10.0 released!",
    "section": "",
    "text": "I want to write up what’s new in version 0.10.0 of VocalPy, released at the end of 2024. Here’s an overview:\nI’ll also give you an overview with this diagram of changes between version 0.9.0 (top ) and version 0.10.0 (bottom).\nThe bottom half of the diagram represents the working model I have of two of the main workflows in VocalPy: segmenting sounds and extracting features. Below I’ll refer back to it, but this post will really focus on improvements and new features. I think diagrams like this are really important to help researchers and software engineers speak the same language—even when those two groups overlap— but I’ll save my thoughts on how to design software for other posts."
  },
  {
    "objectID": "posts/2025-02-14-vocalpy-0.10.0/index.html#bootcamp-driven-development",
    "href": "posts/2025-02-14-vocalpy-0.10.0/index.html#bootcamp-driven-development",
    "title": "VocalPy 0.10.0 released!",
    "section": "Bootcamp-driven development",
    "text": "Bootcamp-driven development\nFirst, some context. I am feeling good about this release because I think it’s a pretty solid step forwards for the package. I’ll tell you why that is, by giving you a brief rundown of the new features and changes, with some narrative that you won’t get from the CHANGELOG. You definitely won’t get that from the auto-generated release notes that GitHub gives us, because they consist of the single commit between this version and the last one. That single commit includes a lot of changes, that corresponds to this pull request, cryptically named “Post-NMAC GRC 2024 fixes”.\nThroughout this post, I’ll link to specific issues on GitHub that were closed by that pull request, to hopefully show you that there’s a method to my madness. About that cryptic name for the pull request: a lot of the features I added and changes I made were after I co-organized and taught this Acoustic Communication And Bioacoustics Bootcamp at the 2024 Neural Mechanisms of Acoustic Communication 2024 Gordon Research Seminar (AKA NMAC GRC 2024, site here), together with the always-excellent Tessa Rhinehart. (Below I’ll call it the ACAB bootcamp for short.)\nI want to give a huge, huge thank you to Nick Jourjine and Diana Liao for inviting Tessa and I to teach this workshop. I know firsthand how important the skills that we taught are, especially for graduate students. It was incredibly gratifying to hear as much from participants in the workshop and other organizers of the conference. If we did nothing else, we pointed people to a lot of resources including the website with a curated database of bioacoustics software that Tessa has created (newly updated just recently with an assist from the research group she’s in), as well as guides for scientists on programming and computational projects that I often point people to when I’m teaching. I think that computational methods in this research area will only continue to become more important (although of course I’m biased, and some might say I’m stating the obvious). I also think Nick has had a lot of foresight in connecting these areas of neuroscience to what people are doing in bioacoustics more broadly, for example with his seminar series “Bridging Brains and Bioacoustics”.\nWithout further ado, here’s a rundown of what’s new in VocalPy 0.10.0.\n\nBetter workflows for making sound clips and segmenting sound\nThe first set of features and changes have to do with two steps we often take when we begin any analysis of animal sounds.\nStep one is to make clips of the sounds we want to analyze. Usually you need to do this because your raw data consists of hours of audio. For example, you have an Automated Recording Unit deployed in the field for passive acoustic monitoring that records some number of hours every day. Or you’re running a behavioral experiment where you have a rig set up to start recording any time the signal from your microphone goes above some level. To make this first step of analysis easier, the Sound class now has a clip method (as planned in issue #149)\nimport vocalpy as voc\n\nsound = voc.Sound.read(\"big-audio-file-01.wav\")\n\n# first clip containing sound of interest\nclip = sound.clip(start=0., stop=1.4)\nclip.write(\"big-audio-file-01-clip-01.wav\")\n\n# second clip containing sound of interest\nclip2 = sound.clip(start=1.4, stop=2.6)\nclip2.write(\"big-audio-file-01-clip-02.wav\")\nNotice in that snippet that we write the clip to a file. That’s because it’s just a Sound; that is, calling clip gives you a new Sound. Hopefully, that isn’t too surprising, but it brings me to the first breaking change: the Sound method no longer has a path attribute (as planned in issue #162). That means that, where before you had\n&gt;&gt;&gt; import vocalpy as voc\n&gt;&gt;&gt; voc.Sound.read(\"samba.wav\")\nSound(data=array[0.001, -0.002, ...], samplerate=44100, path=\"samba.wav\")\nnow you have\n&gt;&gt;&gt; import vocalpy as voc\n&gt;&gt;&gt; voc.Sound.read(\"samba.wav\")\nSound(data=array[0.001, -0.002, ...], samplerate=44100)\nWhy does this matter?\nBecause the design of the class no longer implies that a Sound is tied to the data found in a file at a specific path. This seemingly minor change corresponds to a much bigger change in my mental model of VocalPy and how we work with it as researchers. Again, I’ll save a discussion of that for another post, assuming you’re just here for the “what’s new” part, not a unified theory of research software engineering.\nThat brings us to the second step that we frequently do at the start of an analysis: segment a sequence of sounds into units, for example, the syllables of birdsong. In version 0.10.0, the workflow for this has been significantly improved. Before I tell you how the workflow has been improved, let me give you a little background. One common way to segment sound is with signal processing-based methods. Typically these methods set a threshold on some measure of the energy of an audio signal, and consider any period above that threshold to be a segment (like the functions in vocalpy.segment). A cartoon diagram of this sort of method is shown below.\n\n\n\nFigure 1 of TweetyNet paper showing segmentation\n\n\nMore recently it has become common to use statistical models–more precisely, neural network models for sound event detection such as TweetyNet and DAS. One thing we have noticed is that there is a gap in the literature comparing approaches to segmenting; that’s part of what we’re trying to make it easier to do (for us and for you). This is why we have a vocalpy.metrics.segmentation module. To make it easier to work with segments and compute these metrics, we represent the output of a segmenting algorithm as a data type, vocalpy.Segments. (You might know that some researchers also use image processing methods to segment a spectrogram into a set of boxes; that’s on our to-do list)\nNow that we’re all on the same page about the background, how has the workflow improved? The original design had each of the functions in vocalpy.segment returned a Segments instance, that was a container for a list of separate Segment instances as well as an associated Sound. As of vocalpy 0.10.0, the vocalpy.Segments class no longer has an instance of a vocalpy.Sound associated with it, and there no longer is a separate Segment class (as planned in issue #154). This is another case where I realized life could be a lot easier by simplifying the design of the library. In place of all these tightly-coupled classes, we add a single method to the vocalpy.Sound class, vocalpy.Sound.segment, that takes as its argument an instance of Segments, and returns a list of Sounds (as planned in issue #150). You might notice that you could, if you want, segment using the clip method I just told you about, saving each one of the segments you’re going to analyze as a clip. Again, without going to deeply into a discussion of software design, there’s a couple of drawbacks to that approach: first of all, you will end up taking up a bunch of disk space with duplicated data, and second, you will end up with directories full of tiny files that are hard to navigate if you end up needing to inspect some of your data (and, trust me, you will). Instead, what you want is to be able to represent the segments themselves as data, and then grab the audio clips on the fly. Yes, there are also good reasons to save intermediate steps in your analysis so you don’t have to start over from scratch every time you re-run it —for example, saving spectrograms you generate from the segments of audio— but that’s part of the design discussion I’m glossing over here. The last thing I added to the Segments class was the ability to load a set of segments from a csv file, through a Segments.from_csv method (as planned in issue #186). I realized during the bootcamp we ran that it’s pretty common to save segmentation in csv files, e.g., using the Python library Pandas, and so we should make it easier to load these. (VocalPy saves segments in a json file so we can track metadata like the sampling rate of the segmented audio, but that’s again more detail than I want to get into here.)\nOk, we made it through all the details about segmentation. Last thing: I want to thank Nick Jourjine (again)–talking with him about this kind of workflow really helped me think about how to improve it."
  },
  {
    "objectID": "posts/2025-02-14-vocalpy-0.10.0/index.html#sec-feature-extraction",
    "href": "posts/2025-02-14-vocalpy-0.10.0/index.html#sec-feature-extraction",
    "title": "VocalPy 0.10.0 released!",
    "section": "A simpler, consistent workflow for feature extraction",
    "text": "A simpler, consistent workflow for feature extraction\nOnce I had simplified the workflow for segmenting sounds, I realized I could also simplify the workflow for extracting acoustic features from those sounds. We use a class, FeatureExtractor, to represent this generic step in the workflow; it has an extract method that gives you back Features. This extract method used to take four different things: either a Sound, a Segment (with its associated Sound), a list of Sounds, or a list of Segments. Well, I just told you I removed the Segment class, so obviously the extract method can’t accept a Segment or a list of them anymore. Good, we can remove the logic in extract to handle the Segment class—that’s less to maintain. So I modified the FeatureExtractor class to only take in a Sound or a list of Sounds (as planned in issue #163). There was also an inconsistency in the vocalpy.feature module, where some functions would return Features, and others did not, instead returning a dict mapping feature names to values. In those cases it was the FeatureExtractor taking the output of those functions and making it into a Features instance. So I fixed that inconsistency (as planned in issue #145 and issue #146). Last but not least, I renamed the two feature extraction functions we have right now: from the very verbose vocalpy.features.sat.similarity_features and vocalpy.features.soundsig.predefined_acoustic_features, to just vocalpy.features.sat and vocalpy.features.biosound (as planned in issue #190)."
  },
  {
    "objectID": "posts/2025-02-14-vocalpy-0.10.0/index.html#sec-example-data",
    "href": "posts/2025-02-14-vocalpy-0.10.0/index.html#sec-example-data",
    "title": "VocalPy 0.10.0 released!",
    "section": "More example data that’s easier to work with",
    "text": "More example data that’s easier to work with\nA crucial part of any scientific Python package is having example data, and having it be easy to work with, as I found out the hard way during our live demo at the ACAB bootcamp (protip: always make sure your code is never implicitly assuming you have access to the internet). Thankfully, for most of the Jupyter notebooks we worked through in the bootcamp, we had asked participants to download data ahead of time. In VocalPy 0.10.0, you can now download mini-versions of these datasets (as planned in issue #183), so that you can work through these notebooks, that are now vignettes in the documentation (see next section of this post). Additionally, I added single audio files to use on the two built-in segmenting methods (as planned in issue #182). I also refactored the way examples work, again simplifying things (as planned in issue #185). I’ll spare you the details of what I refactored; the key idea is that calling vocalpy.example with the name of an example will now give you back a VocalPy data type by default, e.g. a Sound. Before you got back a path to a file by default, and you had to do the work of loading data from that file. In the case where it’s one of the datasets with multiple files, you now get back an instance of ExampleData, a sort of Python dictionary where here also the default is the value for each key is a list of VocalPy data types. If you want filenames instead of data types, e.g. to demonstrate functionality for loading data from files, you can instead specify path=True."
  },
  {
    "objectID": "posts/2025-02-14-vocalpy-0.10.0/index.html#sec-vignettes",
    "href": "posts/2025-02-14-vocalpy-0.10.0/index.html#sec-vignettes",
    "title": "VocalPy 0.10.0 released!",
    "section": "Vignettes on using VocalPy with scikit-learn, UMAP, and HDBSCAN",
    "text": "Vignettes on using VocalPy with scikit-learn, UMAP, and HDBSCAN\nOne of the goals of VocalPy is to make it easy to do stuff that’s specific to acoustic communication research, and also get out of the way and make it easy to work with other packages in the scientific Python ecosystem. In the ACAB bootcamp, we worked through a couple of notebooks that show this in action. Specifically, we showed how to fit a classifier in scikit-learn, and how to reduce the dimensionality of a set of animal sounds with UMAP, then cluster the embeddings from UMAP with HDBSCAN. One more thank you to Nick Jourjine, who kindly put together a demo-sized dataset for this second exercise, from the much larger dataset that accompanied his paper on two types of pup calls in deer mice (and let’s also recognize and thank him for actually sharing the code that will make it easier for otherss to build on this work!) In VocalPy 0.10.0, I have added these notebooks as vignettes in the documentation here (as planned in issue #166 and issue #168). I want to be sure to point out that the scikit-learn vignette is adapted from Frederic Theunissen’s BioSound tutorial, because we’re using the set of acoustic features built into their soundsig.BioSound class. The UMAP vignette is adapted from this helpful paper by Thomas et al. and the accompanying code. Again, I’m just trying to make what they’ve done easier for everyone else to do, by focusing a little more on the software engineering side of things."
  },
  {
    "objectID": "posts/2025-02-14-vocalpy-0.10.0/index.html#stay-tuned",
    "href": "posts/2025-02-14-vocalpy-0.10.0/index.html#stay-tuned",
    "title": "VocalPy 0.10.0 released!",
    "section": "Stay tuned",
    "text": "Stay tuned\nHopefully you have a better understanding of how VocalPy is evolving after reading this post. It is now easier to make clips from your raw data, segment your sounds of interest, and extract acoustic features from those sounds. It’s also easier to demonstrate all this functionality with the example data built into VocalPy. Last but not least, it’s easier to see from the docs how you can take what you get out of VocalPy and use it with other Python packages like scikit-learn and UMAP-learn. Having done this development work, for now I will be focusing on new statistical models for jointly segmenting and classifying animal sounds that I’m developing with Yarden Cohen, based on what we’re learning about existing models. (I also want to thank Yarden for continuing our collaboration; without him none of this would be possible.) While I work on those new models and on the next version of VocalPy, we’d love to have your input in our forum. And of course, if you find a bug or you have your own ideas for features, you can raise an issue on the VocalPy repo."
  },
  {
    "objectID": "posts/2025-12-19-domain-driven-design/index.html",
    "href": "posts/2025-12-19-domain-driven-design/index.html",
    "title": "VocalPy as a case study of domain-driven design",
    "section": "",
    "text": "In this post I’m going to talk about domain-driven design, and how I’ve been using that approach when developing VocalPy. In the last post on VocalPy version 0.10.0, I made a point of saying that I would only describe new features, and I’d save discussion of how I designed them for another post. Well, here’s that post.\nI’m writing this post for two audiences. The first is people who are interested in using and/or contributing to VocalPy, that want to better understand its design. The second audience is anyone who is interested in how to design software for scientists, software engineers, and everyone who sits somewhere along the spectrum between those two job titles.\nYou might think I should save my thougts on this until such time that VocalPy actually supports a community of researchers studying acoustic communication. Instead, I should be busy proving this software is actually useful: writing more docs to show how it’s used, adding a bunch of features, recruiting new users and other maintainers. Fair point.\nBut I feel like it’s worth taking just a little bit of time to justify my approach in words, to claim there’s a method to my madness. That’s what I said I’d do in the first post on this blog after all. And reading books like “The Architecture of Open Source Applications” has made me think it’s worth talking about stuff like this."
  },
  {
    "objectID": "posts/2025-12-19-domain-driven-design/index.html#what-is-domain-driven-design",
    "href": "posts/2025-12-19-domain-driven-design/index.html#what-is-domain-driven-design",
    "title": "VocalPy as a case study of domain-driven design",
    "section": "What is domain-driven design?",
    "text": "What is domain-driven design?\nIf you go and read the Forum Acusticum proceedings paper, “Introducing VocalPy”, you’ll see that I name-drop domain-driven design there. But I don’t talk about it a lot, since you only get so much space in a proceedings paper. Please consider this a longer, much more informal, version of what I wrote there.\nFor this post to make sense, I have to introduce domain-driven design. I also need to repeat myself a little – I wrote about this on my personal blog. If you came here from there, feel free to skip down to where I start talking about how I’ve used domain-driven design when developing VocalPy.\nWhat is domain-driven design, and why should you care about it? Sometime in 2022-2023, I read Domain-Driven Design by Eric Evans, and I got really excited about it. (You can get it from bookshop.org here, and if you’re feeling dangerous you can probably find a PDF of it on a random GitHub repository.) I ended up reading it because I had been reading Architecture Patterns with Python, and they mentioned it in the introduction.\nIf you do nothing else, read the first chapter of Evans’ book, where he relates the story of how he worked with some electrical engineers to design software they would use to design printed circuit boards (AKA PCBs).\nAt the beginning, he makes mistakes. He tries to understand their jargon word-for-word. Then he asks them to specify in detail what they think the software should do. Neither of those approaches were ever going to work well. Finally he hits upon the idea of asking them to draw out diagrams of their process and how the software should interact with it. These are simple, rough box and arrow sketches as he shows.\n\n\n\nevans figure 1.2\n\n\nNotice what is happening here: this is not just a developer creating a UML diagram to show to other developers. This is software engineers and domain experts developing a pidgin language together. They use this pidgin to talk about the domain problem they are trying to solve with software.\nIt’s an interesting story for a couple of reasons. First of all, you have a feeling that he is almost an anthropologist, going into this unfamiliar tribe of electrical engineers so he can learn their culture. I think this is a familiar feeling for anyone who has tried to translate some real-world domain into software, even if it’s part of a culture they feel like they belong to. Second, you really get a feel for his process.\nIf you have ever gone through the process of designing software for some real-world domain, I bet the story really resonates with you."
  },
  {
    "objectID": "posts/2025-12-19-domain-driven-design/index.html#domain-driven-vocalpy",
    "href": "posts/2025-12-19-domain-driven-design/index.html#domain-driven-vocalpy",
    "title": "VocalPy as a case study of domain-driven design",
    "section": "How I’ve used domain-driven design when developing VocalPy",
    "text": "How I’ve used domain-driven design when developing VocalPy\nI happened to read Evans’ book at the same time that I had been sketching out some initial ideas for VocalPy. You can see some of these sketches here: https://github.com/vocalpy/vocalpy/issues/19\n\n\n\nschematic of library named vocles from linked github issue\n\n\nIf you were to click through to the library’s docs, you might notice that these bear little resemblance to VocalPy now. I think this is actually a good thing – more on that below. (You might also notice at the time I was thinking of calling it vocles – everyone please clap for me showing enough restraint for once in my life to not make a bad pun.)\nI don’t actually remember which came first: these sketches, or me reading the book. I think that I actually drew the sketches first, and had them sitting around on a desk forever, until finally it hit me that I should add them to the repo to document my design process. And then reading this part of Evans’ book really made me think that drawings like this should be integral to the design process. Part of what I want to say here is that, you should be doing this, if you’re not already, and what’s more, you should be including it in your docs for your software. And this goes for all software, unless you are literally writing such a boring cookiecutter CRUD app that a so-called Large Language Model can regurgitate it perfectly for you, after being “trained” on the actual work of human beings. I would argue that these drawings should be part of the theory behind your scientific software.\nOk, now you have an idea of what domain-driven design is. You might ask yourself, have I done anything with this? Or do I just like pontificating into the void about ideas from computer science and tech books? Even if I haven’t gone back to finish the book and immersed myself in every detail, the core idea has really stuck with me. Going back to that Proceedings paper where I first introduced VocalPy, you can see where I included a similar drawing, cleaned up for a paper.\n\n\n\nfigure 1 from VocalPy proceedings paper\n\n\nEven by the time I got to this first Proceeding paper, the design of the library had evolved. But this is a good thing — I did exactly what Evans prescribed, and continued to iterate on the design of the package. Doing so made me realized which parts were actually useful, that I wanted to retain in the core. I think sketching things out has also helped me understand why the things I ended up taking out are still useful, just not in the way I had thought at first. The library at first was very focused on the idea of capturing a dataset of specific file types, and then being able to save this dataset in the form of a SQLite file. You can see where I was really focused on treating the dataset as if it were part of an app, like in the architecture book. I do think this is still important, but it is not the core of what the library does – I realized later that the core data types needed to be things like sounds, spectrograms, annotations, the things that a researcher studying animal communication and using bioacoustics would be talking about. So, basically, I did the anthropological exercise, as in Chapter 1 of Evans’ book, but instead of doing it with other people, I started by doing it with the part of my brain that claims to know things about acoustic communication. (I have since engaged with other people who actually know these things and can give me good feedback.)\nYou can also see how I’ve continued this way, looking at the diagram I show at the top of that last post on VocalPy version 0.10.0\n\n\n\nDiagram showing changes between versions 0.9.0 and 0.10.0 of VocalPy\n\n\nMaking these diagrams helped me understand how to change the key workflow in VocalPy. Basically, I realized that I could remove the abstraction of a single Segment, and replace it with a Sound.segment method. That segment method takes in the output of a segmenting algorithm, represented as an instance of Segments, and then returns a list of Sounds, one for each segment.\nThis lets me still preserve the outputs of a segmenting algorithm for downstream analysis, while simplifying the library.\nThe simplification minimizes the cogitive load on users and developers, who no longer have to remember how a Segment is related to Segments and how to get a Sound from a Segment. (For more details on what changed, please see that post. Again, realizing that I could make this change by sketching out the workflows is, in my understanding at least, exactly the kind of iterative development process that Evans advocates for in his book."
  },
  {
    "objectID": "posts/2025-12-19-domain-driven-design/index.html#if-domain-driven-design-is-just-doodles-then-you-should-put-doodles-in-your-docs",
    "href": "posts/2025-12-19-domain-driven-design/index.html#if-domain-driven-design-is-just-doodles-then-you-should-put-doodles-in-your-docs",
    "title": "VocalPy as a case study of domain-driven design",
    "section": "If domain-driven design is just doodles, then you should put doodles in your docs",
    "text": "If domain-driven design is just doodles, then you should put doodles in your docs\nHaving read this, you might feel like: “Cool story bro, but domain-driven design is not a new idea. I already do that.” I talk about that more in that post on my personal blog. To sum it up, here I’ll say, sure, domain-driven design is not a new idea (as Evans himself acknowledges right at the start of his book). But I also want to use what I’ve said about VocalPy to make the case once more for why I think we should be doing more of it.\nHere I want to link to a talk I gave, fittingly titled “VocalPy as a case study of domain-driven design in scientific Python”. (Huge thank you to the DoePy exchange and Don’t Use this Code for inviting me and giving me space to talk through these things with a sympathetic community.)\n\n\nOne of the things I explain in the talk is how my attempt to apply domain-driven design has led me to do things that maybe clash with some recommendations for programming in scientific Python.\nCompare for example with Gaël Varoquaux’s recommendations in this SciPy 2017 talk:\n\n\nThe main thing I am doing that would be considered a no-no is: not restricting myself to using the core types of the scientific Python stack, e.g., the NumPy array and the pandas DataFrame. Instead, I have a core set of types in terms of the domain, bioacoustics and acoustic communication in animals: Sounds, Spectrograms, (acoustic) Features, etc. These types are basically data containers, where the underlying array or dataframe is just one attribute away, often literally named data, i.e., Sound.data. My argument is that this is a small price to pay,\nif it makes the code more immediately readable to other researchers in my domains of interest.\nThere are other recommendations that Gaël makes in his talk that I should probably take to heart. I can think of some classes in VocalPy that probably should just be functions. Please don’t think I’m claiming that domain-driven design is some magic method you can follow to produce correct research code. I definitely do not claim to be any smarter or more experienced of a developer than Gaël Varoquaux. This is just me trying to wrap my head around designing software for different domains, and how to reconcile that with different programming paradigms.\n(Yes, this is foreshadowing for another blog post.)\nIn the discussion at the end of that talk, I said just what I’ve said here, that a lot of people react as if, “so, yeah, we already do that”. If that’s so, then show me the doodles! Show me your mental model of your domain — put it in your docs! Let me read it, let me actually see these schematics, even if they are just doodles, it helps me to know how your thought process evolved. All I can see right now is this insurmountable mountain of code, and I don’t even know where the path starts so I can scale it! I know that there are examples of people doing this, e.g., in the scientific Python community where I spend most of my time, but I think it’s fair to say that this is not the norm. I don’t know that I have ever seen diagrams showing how the design evolved, as part of an iterative development process. I can’t help but feel like that’s exactly the sort of thing that could help people get up to speed on how the code works. Again: I tried to give some concrete examples of this in that post on my personal blog. But I hope that here I’ve given you some idea of how I use domain-driven design to develop VocalPy, and more generally what domain-driven design for research code can look like."
  },
  {
    "objectID": "posts/2025-02-11-developers-blog/index.html",
    "href": "posts/2025-02-11-developers-blog/index.html",
    "title": "Okay, fine, we’ll have a developer’s blog",
    "section": "",
    "text": "Hey, it’s me, David.\nI’m the person who develops most of this code. I’m interested in how animals communicate with sound, and I’m trying to make life easier for all of us that use computational methods to study acoustic behavior.\nHopefully, at some point, a team of people will work together to develop it, or something like it, once I can convince them that’s a good idea.\nThen finally I can spend more time playing video games and birding, or doing whatever it is that people do when they’re not being open source software maintainers.\nFirst I have to convince more people that the ideas behind this code are good ideas. Recently I have realized that just writing the code isn’t enough. According to people who have been at this longer than I have, one way to talk people into things is to build not just a project, but a community. (See rule 5 here, and notice I am ignoring rule 1.) Well, I have been working on building a community:\nwe have gotten a lot of contributors to at least one package, vak. And I am expecting to get more contributors to our core package VocalPy, once we share results we are getting with it right now, using newer features we have added and are adding. We also have a growing number of people joining our forum and asking questions.\nBut it seems like I need to do more if I really want the community to grow. I need to actually explain about what I’m doing here.\nSo, okay, fine, we will have a developer’s blog. And I guess right now that’s the royal we–mostly me, although I definitely don’t want it to stay that way. Mainly this will be a place to post about new releases, and about research that uses the code. Look for more posts soon."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "VocalPy blog",
    "section": "",
    "text": "VocalPy as a case study of domain-driven design\n\n\n\n\n\n\nVocalPy\n\n\ndomain-driven design\n\n\n\nHow I’m using domain-driven design to develop VocalPy\n\n\n\n\n\nDec 19, 2025\n\n\nDavid Nicholson\n\n\n\n\n\n\n\n\n\n\n\n\nVocalPy 0.10.0 released!\n\n\n\n\n\n\nVocalPy\n\n\nrelease\n\n\n\nWhat’s new in VocalPy version 0.10.0\n\n\n\n\n\nFeb 14, 2024\n\n\nDavid Nicholson\n\n\n\n\n\n\n\n\n\n\n\n\nOkay, fine, we’ll have a developer’s blog\n\n\n\n\n\n\nvocalpy\n\n\nblog\n\n\n\nObligatory first post about blog\n\n\n\n\n\nFeb 11, 2024\n\n\nDavid Nicholson\n\n\n\n\n\n\nNo matching items"
  }
]